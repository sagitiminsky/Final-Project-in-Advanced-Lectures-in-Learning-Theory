{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLWfTDxfl8+7ZN+XWEuYu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagitiminsky/Final-Project-in-Advanced-Lectures-in-Learning-Theory/blob/part1/part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XQp5qWm-l0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "import torchvision.transforms\n",
        "import copy\n",
        "!pip3 install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os"
      ],
      "metadata": {
        "id": "nr1zFGF6l5Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f98321-bac0-4757-c85d-61479a5d55b0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.10)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.26)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.5)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msagit\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources\n",
        "\n",
        "\n",
        "1.   GD - https://www.linkedin.com/pulse/pytorch-gradient-descent-stochastic-mini-batch-code-sobh-phd/\n",
        "2.  Constrained GD - https://botorch.org/tutorials/optimize_stochastic\n",
        "\n"
      ],
      "metadata": {
        "id": "rtg8c0EyBeRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Drive"
      ],
      "metadata": {
        "id": "LmsW6UHfoi0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLgJxQkoqNV",
        "outputId": "9d479dbe-f5c6-43d8-cd90-42d1da75bd8a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root=\"/content/drive/MyDrive/university/Masters/datasets\""
      ],
      "metadata": {
        "id": "B4bI2jNDpiYs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import MNIST Dataset\n",
        "\n",
        "\n",
        "*   According to the assignment, we should construct one big data set (which you will later divide to train-test by our selves.\n",
        "*   We will preform the following transformations: Flip the image horizontally and then rotate it 90 degrees anti-clockwise. Without if the images are not alligend correctly which might suboptimal results in run-time (We assume that the images proviveded in run time are alligned correctly)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lTjZ5XkQllUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=torchvision.transforms.Compose([\n",
        "                    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                    lambda img: torchvision.transforms.functional.hflip(img)\n",
        "                ])\n",
        "\n",
        "mnist_trainset = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform)\n",
        "mnist_testset = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform)\n",
        "\n",
        "mnist_dataset=mnist_trainset + mnist_testset"
      ],
      "metadata": {
        "id": "isIoQD3dlr9L"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_a_few_samples(dataset,n):\n",
        "  for i in range(n):\n",
        "    train_image_sample, train_target_sample = dataset[i]\n",
        "    # display(train_image_sample)\n",
        "    print(f'GT: {train_target_sample}')\n",
        "    # print(f'size: {train_image_sample.size}')\n",
        "\n",
        "visualize_a_few_samples(mnist_testset,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OeUJ67lvCla",
        "outputId": "37c12640-20df-4183-d199-619ff6983ea1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT: 1\n",
            "GT: 8\n",
            "GT: 6\n",
            "GT: 7\n",
            "GT: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and creting 3 different classification problems\n",
        "\n",
        "The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset, we'll take them as a given here."
      ],
      "metadata": {
        "id": "KPDgjS19HdNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "transform=torchvision.transforms.Compose([\n",
        "                    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                    lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307), (0.3081))\n",
        "                ])"
      ],
      "metadata": {
        "id": "LG-R3C8j4Gce"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the label 0 to all examples labeled [0,1,2,3,4] and the label 1 to all examples labeled [5,6,7,8,9]\n",
        "\n",
        "def zero_to_four_vs_five_to_nine(number):\n",
        "  if number>=0 and number<=4:\n",
        "    res=torch.tensor([0]).to(torch.float32)\n",
        "  elif number>=5 and number<=9:\n",
        "     res=torch.tensor([1]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "  return res\n",
        "\n",
        "mnist_trainset_zero_to_four_vs_five_to_nine = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=zero_to_four_vs_five_to_nine)\n",
        "mnist_testset_zero_to_four_vs_five_to_nine = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=zero_to_four_vs_five_to_nine)\n",
        "\n",
        "mnist_dataset_zero_to_four_vs_five_to_nine = mnist_trainset_zero_to_four_vs_five_to_nine + mnist_testset_zero_to_four_vs_five_to_nine\n",
        "\n",
        "visualize_a_few_samples(mnist_testset_zero_to_four_vs_five_to_nine, 20)"
      ],
      "metadata": {
        "id": "_EnujD1pHh1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7592fe8-962f-4194-a7db-97eb56907726"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the lable 1 to even numbers, and the label 0 to odd numbers\n",
        "def even_vs_odd(number):\n",
        "  if number>=0 and number<=9:\n",
        "    return torch.tensor([number%2]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "\n",
        "mnist_trainset_even_vs_odd = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=even_vs_odd)\n",
        "mnist_testset_even_vs_odd = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=even_vs_odd)\n",
        "\n",
        "mnist_dataset_even_vs_odd = mnist_trainset_even_vs_odd + mnist_testset_even_vs_odd\n",
        "\n",
        "#visualize_a_few_samples(mnist_dataset_even_vs_odd, 5)"
      ],
      "metadata": {
        "id": "4P0IM1RjH28j"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the lable 1 to primary numbers, and the label 0 to non primary numbers\n",
        "def prime_vs_not_prime(number):\n",
        "  is_prime=[1,2,3,5,7]\n",
        "  if number>=0 and number<=9:\n",
        "    if number in is_prime:\n",
        "      return torch.tensor([1]).to(torch.float32)\n",
        "    else:\n",
        "      return torch.tensor([0]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "\n",
        "mnist_trainset_prime_vs_not_prime = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=prime_vs_not_prime)\n",
        "mnist_testset_prime_vs_not_prime = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=prime_vs_not_prime)\n",
        "\n",
        "mnist_dataset_prime_vs_not_prime = mnist_trainset_prime_vs_not_prime + mnist_testset_prime_vs_not_prime\n",
        "\n",
        "#visualize_a_few_samples(mnist_dataset_prime_vs_not_prime, 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "uCg3FXVOIEi9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment A: Optimization\n",
        "*   In order to keep our experiments clean and tidy, we are going to visualize them in WandB\n",
        "*   As mentioned we are going to use only the traning set in this section for each classification problem\n",
        "\n"
      ],
      "metadata": {
        "id": "B9OwpZD3X0uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the lecture notes and what we studied in class, choose “theoretically justified” learning rates and other parameters (such as λ in\n",
        "regularized GD). State your choice and provide the justification.\n"
      ],
      "metadata": {
        "id": "LlZSVg07d6gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "One of the most important parameters to select is the learning rate. Adapting the learning rate on trainer init and as the model trains can reduce training time and avoiding local minimums.\n",
        "\n",
        "*   The common approach is to start with a small learning rate and increase it exponentially if two apochs in a row reduce the rror.\n",
        "*   Another approach relies on gradient direction - again we start with a small learning rate but this time we increase it if for two epochs in row the gradient direction is similar, otherwise decrease it if the direcctions differs.\n",
        "\n",
        "Two other important parametrs are the depth(number of layers) and the width(number of neurons) of the nueral network. If adding a new layer does not provide significate decrease in the training error then there most liklely is not need to add more layers. Same goes for the network's width.\n",
        "\n"
      ],
      "metadata": {
        "id": "MbbUcfvDeUjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(n_feature, n_hidden),\n",
        "            torch.nn.Linear(n_hidden, n_output)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n"
      ],
      "metadata": {
        "id": "IVbIe6JQexoQ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(experiment,optimizer_name, epoch,lr,weight_decay):\n",
        "  network.train()\n",
        "  optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
        "  targets=torch.LongTensor([target for feature,target in experiment]).to(device)\n",
        "  if optimizer_name=='gd':\n",
        "    predictions=network(experiment.data.to(device).float())\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='constrained_gd':\n",
        "    predictions=network(experiment.data.to(device).float())\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      for param in network.parameters():\n",
        "          param.clamp_(-1, 1)\n",
        "      \n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='regularized_gd':\n",
        "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, weight_decay=weight_decay) \n",
        "    predictions=network(experiment.data.to(device).float())\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='sgd':\n",
        "    for i in range(len(experiment)):\n",
        "      optimizer.zero_grad()\n",
        "      data,target=experiment[i]\n",
        "      data=data.to(device).float()\n",
        "      target=target.type(torch.LongTensor)\n",
        "      prediction = network(data)\n",
        "      loss = F.nll_loss(prediction, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  else:\n",
        "    raise NotImplementedError(f\"This optimizer:{optimizer} is not impelemtned\")\n",
        "  "
      ],
      "metadata": {
        "id": "cix3bp5hjDSm"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "\n",
        "n_epochs = 1\n",
        "batch_size_train = 64\n",
        "momentum = 0.5\n",
        "log_interval = 1\n",
        "lrs=[1e-4]\n",
        "weight_decays=[1e-5]\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "optimizers_names = ['gd','constrained_gd','regularized_gd', 'sgd'] \n",
        "experimetns = {\n",
        "    \"zero_to_four_vs_five_to_nine\": mnist_testset_zero_to_four_vs_five_to_nine,\n",
        "    \"even_vs_odd\": mnist_testset_even_vs_odd,\n",
        "    \"prime_vs_not_prime\": mnist_testset_prime_vs_not_prime\n",
        "}\n",
        "\n",
        "for experiment in experimetns:\n",
        "  for optimizer_name in optimizers_names:\n",
        "    for lr in lrs:\n",
        "      for weight_decay in weight_decays:\n",
        "          print(f\"experiment:{experiment} | optimizer_name:{optimizer_name}\")\n",
        "          network = Net(n_feature=784, n_hidden=10, n_output=2)\n",
        "          # run=wandb.init(project=\"Final Project in Advanced Lectures in Learning\", entity=\"sagit\", name=f\"experiment:{experiment} | optimizer_name:{optimizer_name} | lr:{lr} | weight_decay:{weight_decay}\")\n",
        "          # wandb.watch(network, log_freq=1)\n",
        "          for epoch in range(1, n_epochs + 1):\n",
        "            loss=train(experimetns[experiment],optimizer_name, epoch,lr, weight_decay)\n",
        "            if epoch % log_interval==0:\n",
        "              print(f\"loss:{loss}\")\n",
        "            #   wandb.log({\n",
        "            #       \"training_loss\": loss\n",
        "            #   })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "GqDlwznGi7je",
        "outputId": "4bf86271-de43-4c0a-db34-44e4161fd50f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:gd\n",
            "target: tensor([0, 1, 1,  ..., 1, 1, 1])\n",
            "loss:-4.70602560043335\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:constrained_gd\n",
            "loss:-8.717506408691406\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:regularized_gd\n",
            "loss:-5.104849815368652\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:sgd\n",
            "loss:-9659480064.0\n",
            "experiment:even_vs_odd | optimizer_name:gd\n",
            "target: tensor([1, 0, 0,  ..., 0, 1, 0])\n",
            "loss:29.127120971679688\n",
            "experiment:even_vs_odd | optimizer_name:constrained_gd\n",
            "loss:6.613851547241211\n",
            "experiment:even_vs_odd | optimizer_name:regularized_gd\n",
            "loss:16.313018798828125\n",
            "experiment:even_vs_odd | optimizer_name:sgd\n",
            "loss:-12555416576.0\n",
            "experiment:prime_vs_not_prime | optimizer_name:gd\n",
            "target: tensor([1, 0, 0,  ..., 0, 0, 0])\n",
            "loss:-1.6395070552825928\n",
            "experiment:prime_vs_not_prime | optimizer_name:constrained_gd\n",
            "loss:20.94171714782715\n",
            "experiment:prime_vs_not_prime | optimizer_name:regularized_gd\n",
            "loss:5.64408016204834\n",
            "experiment:prime_vs_not_prime | optimizer_name:sgd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-8b1e84239b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0;31m# wandb.watch(network, log_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimetns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss:{loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-875b98db82df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(experiment, optimizer_name, epoch, lr, weight_decay)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_enter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XsIQ-A64kfcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}