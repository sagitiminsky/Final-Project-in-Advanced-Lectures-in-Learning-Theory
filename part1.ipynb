{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLcEmUeIP9O4J3COEGt1yK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagitiminsky/Final-Project-in-Advanced-Lectures-in-Learning-Theory/blob/part1/part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XQp5qWm-l0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "import torchvision.transforms\n",
        "import copy\n",
        "!pip3 install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os"
      ],
      "metadata": {
        "id": "nr1zFGF6l5Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e75cf7-777b-48ff-c97a-6ace077f0eee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 890 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 921 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 993 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.5-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=590347a0693481a3c6c943296cb15a2b523e39f9bdbbd0a86cef5c9597923e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.26 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.5 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources\n",
        "\n",
        "\n",
        "1.   GD - https://www.linkedin.com/pulse/pytorch-gradient-descent-stochastic-mini-batch-code-sobh-phd/\n",
        "2.  Constrained GD - https://botorch.org/tutorials/optimize_stochastic\n",
        "\n"
      ],
      "metadata": {
        "id": "rtg8c0EyBeRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Drive"
      ],
      "metadata": {
        "id": "LmsW6UHfoi0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLgJxQkoqNV",
        "outputId": "3f452f4d-0b63-4436-d10e-f2878d574310"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root=\"/content/drive/MyDrive/university/Masters/datasets\""
      ],
      "metadata": {
        "id": "B4bI2jNDpiYs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import MNIST Dataset\n",
        "\n",
        "\n",
        "*   According to the assignment, we should construct one big data set (which you will later divide to train-test by our selves.\n",
        "*   We will preform the following transformations: Flip the image horizontally and then rotate it 90 degrees anti-clockwise. Without if the images are not alligend correctly which might suboptimal results in run-time (We assume that the images proviveded in run time are alligned correctly)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lTjZ5XkQllUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=torchvision.transforms.Compose([\n",
        "                    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                    lambda img: torchvision.transforms.functional.hflip(img)\n",
        "                ])\n",
        "\n",
        "mnist_trainset = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform)\n",
        "mnist_testset = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform)\n",
        "\n",
        "mnist_dataset=mnist_trainset + mnist_testset"
      ],
      "metadata": {
        "id": "isIoQD3dlr9L"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_a_few_samples(dataset,n):\n",
        "  for i in range(n):\n",
        "    train_image_sample, train_target_sample = dataset[i]\n",
        "    # display(train_image_sample)\n",
        "    print(f'GT: {train_target_sample}')\n",
        "    # print(f'size: {train_image_sample.size}')\n",
        "\n",
        "visualize_a_few_samples(mnist_testset,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OeUJ67lvCla",
        "outputId": "6dec2e77-0e0a-415e-cfa7-0770b7be3549"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT: 1\n",
            "GT: 8\n",
            "GT: 6\n",
            "GT: 7\n",
            "GT: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and creting 3 different classification problems\n",
        "\n",
        "The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset, we'll take them as a given here."
      ],
      "metadata": {
        "id": "KPDgjS19HdNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "transform=torchvision.transforms.Compose([\n",
        "                    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                    lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307), (0.3081))\n",
        "                ])"
      ],
      "metadata": {
        "id": "LG-R3C8j4Gce"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the label 0 to all examples labeled [0,1,2,3,4] and the label 1 to all examples labeled [5,6,7,8,9]\n",
        "\n",
        "def zero_to_four_vs_five_to_nine(number):\n",
        "  if number>=0 and number<=4:\n",
        "    res=torch.tensor([0]).to(torch.float32)\n",
        "  elif number>=5 and number<=9:\n",
        "     res=torch.tensor([1]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "  return res\n",
        "\n",
        "mnist_trainset_zero_to_four_vs_five_to_nine = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=zero_to_four_vs_five_to_nine)\n",
        "mnist_testset_zero_to_four_vs_five_to_nine = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=zero_to_four_vs_five_to_nine)\n",
        "\n",
        "mnist_dataset_zero_to_four_vs_five_to_nine = mnist_trainset_zero_to_four_vs_five_to_nine + mnist_testset_zero_to_four_vs_five_to_nine\n",
        "\n",
        "visualize_a_few_samples(mnist_testset_zero_to_four_vs_five_to_nine, 20)"
      ],
      "metadata": {
        "id": "_EnujD1pHh1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f46b66-2bb0-4e18-fb3c-f4ce92f1aa6c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([1.])\n",
            "GT: tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the lable 1 to even numbers, and the label 0 to odd numbers\n",
        "def even_vs_odd(number):\n",
        "  if number>=0 and number<=9:\n",
        "    return torch.tensor([number%2]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "\n",
        "mnist_trainset_even_vs_odd = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=even_vs_odd)\n",
        "mnist_testset_even_vs_odd = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=even_vs_odd)\n",
        "\n",
        "mnist_dataset_even_vs_odd = mnist_trainset_even_vs_odd + mnist_testset_even_vs_odd\n",
        "\n",
        "#visualize_a_few_samples(mnist_dataset_even_vs_odd, 5)"
      ],
      "metadata": {
        "id": "4P0IM1RjH28j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we assign the lable 1 to primary numbers, and the label 0 to non primary numbers\n",
        "def prime_vs_not_prime(number):\n",
        "  is_prime=[1,2,3,5,7]\n",
        "  if number>=0 and number<=9:\n",
        "    if number in is_prime:\n",
        "      return torch.tensor([1]).to(torch.float32)\n",
        "    else:\n",
        "      return torch.tensor([0]).to(torch.float32)\n",
        "  else:\n",
        "    raise Exception(f\"{number} is not a valid target in the dataset\")\n",
        "\n",
        "\n",
        "mnist_trainset_prime_vs_not_prime = datasets.EMNIST(root=root, train=True, split=\"mnist\",download=True, transform=transform, target_transform=prime_vs_not_prime)\n",
        "mnist_testset_prime_vs_not_prime = datasets.EMNIST(root=root, train=False, split=\"mnist\",download=True, transform=transform, target_transform=prime_vs_not_prime)\n",
        "\n",
        "mnist_dataset_prime_vs_not_prime = mnist_trainset_prime_vs_not_prime + mnist_testset_prime_vs_not_prime\n",
        "\n",
        "#visualize_a_few_samples(mnist_dataset_prime_vs_not_prime, 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "uCg3FXVOIEi9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment A: Optimization\n",
        "*   In order to keep our experiments clean and tidy, we are going to visualize them in WandB\n",
        "*   As mentioned we are going to use only the traning set in this section for each classification problem\n",
        "\n"
      ],
      "metadata": {
        "id": "B9OwpZD3X0uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the lecture notes and what we studied in class, choose “theoretically justified” learning rates and other parameters (such as λ in\n",
        "regularized GD). State your choice and provide the justification.\n"
      ],
      "metadata": {
        "id": "LlZSVg07d6gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "One of the most important parameters to select is the learning rate. Adapting the learning rate on trainer init and as the model trains can reduce training time and avoiding local minimums.\n",
        "\n",
        "*   The common approach is to start with a small learning rate and increase it exponentially if two apochs in a row reduce the rror.\n",
        "*   Another approach relies on gradient direction - again we start with a small learning rate but this time we increase it if for two epochs in row the gradient direction is similar, otherwise decrease it if the direcctions differs.\n",
        "\n",
        "Two other important parametrs are the depth(number of layers) and the width(number of neurons) of the nueral network. If adding a new layer does not provide significate decrease in the training error then there most liklely is not need to add more layers. Same goes for the network's width.\n",
        "\n"
      ],
      "metadata": {
        "id": "MbbUcfvDeUjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(n_feature, n_hidden),\n",
        "            torch.nn.Linear(n_hidden, n_output)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n"
      ],
      "metadata": {
        "id": "IVbIe6JQexoQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(experiment,optimizer_name, epoch,lr,weight_decay):\n",
        "  network.train()\n",
        "  optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
        "  targets=torch.LongTensor([target for feature,target in experiment]).to(device)\n",
        "  if optimizer_name=='gd':\n",
        "    predictions=network(experiment.data.to(device).float())\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='constrained_gd':\n",
        "    predictions=network(experiment.data)\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    # optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      for param in network.parameters():\n",
        "          param.clamp_(-1, 1)\n",
        "      \n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='regularized_gd':\n",
        "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, weight_decay=weight_decay) \n",
        "    predictions=network(experiment.data)\n",
        "    loss = F.nll_loss(predictions, targets)\n",
        "    # optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  elif optimizer_name=='sgd':\n",
        "    for batch_idx, (data, target) in enumerate(experiment):\n",
        "      # optimizer.zero_grad()\n",
        "      output = network(data)\n",
        "      loss = F.nll_loss(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  else:\n",
        "    raise NotImplementedError(f\"This optimizer:{optimizer} is not impelemtned\")\n",
        "  "
      ],
      "metadata": {
        "id": "cix3bp5hjDSm"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "#DEBUGGING\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "n_epochs = 15\n",
        "batch_size_train = 64\n",
        "momentum = 0.5\n",
        "log_interval = 1\n",
        "lrs=[1e-4]\n",
        "weight_decays=[1e-5]\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "optimizers_names = ['gd','constrained_gd','regularized_gd','sgd'] \n",
        "experimetns = {\n",
        "    \"zero_to_four_vs_five_to_nine\": mnist_testset_zero_to_four_vs_five_to_nine,\n",
        "    \"even_vs_odd\": mnist_testset_even_vs_odd,\n",
        "    \"prime_vs_not_prime\": mnist_testset_prime_vs_not_prime\n",
        "}\n",
        "\n",
        "for experiment in experimetns:\n",
        "  for optimizer_name in optimizers_names:\n",
        "    for lr in lrs:\n",
        "      for weight_decay in weight_decays:\n",
        "          print(f\"experiment:{experiment} | optimizer_name:{optimizer_name}\")\n",
        "          network = Net(n_feature=784, n_hidden=10, n_output=2)\n",
        "          # run=wandb.init(project=\"Final Project in Advanced Lectures in Learning\", entity=\"sagit\", name=f\"experiment:{experiment} | optimizer_name:{optimizer_name} | lr:{lr} | weight_decay:{weight_decay}\")\n",
        "          # wandb.watch(network, log_freq=1)\n",
        "          for epoch in range(1, n_epochs + 1):\n",
        "            loss=train(experimetns[experiment],optimizer_name, epoch,lr, weight_decay)\n",
        "            if epoch % log_interval==0:\n",
        "              print(f\"loss:{loss}\")\n",
        "            #   wandb.log({\n",
        "            #       \"training_loss\": loss\n",
        "            #   })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "GqDlwznGi7je",
        "outputId": "358a6323-5beb-421b-fce8-be6d9434f790"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:gd\n",
            "loss:-4.70602560043335\n",
            "loss:-27.673643112182617\n",
            "loss:-51.65351104736328\n",
            "loss:-78.0307846069336\n",
            "loss:-108.29627227783203\n",
            "loss:-144.13525390625\n",
            "loss:-187.52586364746094\n",
            "loss:-240.85215759277344\n",
            "loss:-307.03924560546875\n",
            "loss:-389.7174377441406\n",
            "loss:-493.4249572753906\n",
            "loss:-623.8609619140625\n",
            "loss:-788.201904296875\n",
            "loss:-995.5003051757812\n",
            "loss:-1257.186767578125\n",
            "experiment:zero_to_four_vs_five_to_nine | optimizer_name:constrained_gd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-b142295369fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0;31m# wandb.watch(network, log_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimetns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss:{loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-154224f2d59c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(experiment, optimizer_name, epoch, lr, weight_decay)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'constrained_gd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-74a8e05d39be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XsIQ-A64kfcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}